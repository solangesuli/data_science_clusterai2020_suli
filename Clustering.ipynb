{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerías necesarias para trabajar.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# importamos las librerías para el preprocesing y PCA.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# importamos las librerías gráficas.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Importamos las librerias para Clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score\n",
    "\n",
    "#Otros \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caba = pd.read_feather('./data/CovidCabaFeather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'preproc' es el parámetro que define el número de features que usaremos como input para el experimento.\n",
    "# [6, 30, 'raw'] es la lista de parámetros que usaremos para desarrollar el presente trabajo.\n",
    "#\n",
    "# * 6 y 30, o cualquier número entero entre 1 y el número total de features en el dataset original (65), \n",
    "#   será la dimensión de nuestros samples para el proceso de clustering.\n",
    "# * 'raw' es el único parámetro fuera de los antes mencionados que acepta el notebook y provoca que se usen los\n",
    "#   features originales, evitando el proceso de PCA.\n",
    "\n",
    "preproc = 6\n",
    "\n",
    "# 'n_clusters' es el número de clusters que generará el notebook.\n",
    "# [2, 4, 6] es el set de parámetros que usaremos para este trabajo.\n",
    "#\n",
    "# * Puede usarse cualquier número entero entre 1 y 6, eligiéndose este máximo para facilitar la visualización\n",
    "#   de los resultados.\n",
    "\n",
    "n_clusters = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos desacemos de las columnas que no formarán parte del estudio.\n",
    "\n",
    "for columna in ['index', \n",
    "                'id_evento_caso', \n",
    "                'fecha_inicio_sintomas', \n",
    "                'fecha_apertura', \n",
    "                'fecha_internacion', \n",
    "                'fecha_cui_intensivo', \n",
    "                'fecha_fallecimiento',\n",
    "                'carga_provincia_id',\n",
    "                'fecha_diagnostico',\n",
    "                'residencia_provincia_id',\n",
    "                'residencia_departamento_id'\n",
    "               ]: \n",
    "    df_caba.pop(columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generamos las que serán nuestras columnas dato.\n",
    "\n",
    "df_caba.loc[df_caba.edad_años_meses == 'Meses', 'edad'] = df_caba.edad / 12\n",
    "for columna in ['edad_años_meses']: df_caba.pop(columna) \n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['sexo'])], axis=1)\n",
    "df_caba.rename(columns = {'M':'Masculino', 'F':'Femenino'}, inplace = True)\n",
    "for columna in ['sexo', 'NR']: df_caba.pop(columna)\n",
    "    \n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['residencia_pais_nombre'])], axis=1)\n",
    "for columna in ['Bolivia', 'Paraguay', 'SIN ESPECIFICAR', 'Venezuela', 'residencia_pais_nombre']: df_caba.pop(columna) \n",
    "df_caba.rename(columns = {'Argentina':'residencia_Argentina'}, inplace = True)\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['carga_provincia_nombre'])], axis=1)\n",
    "for columna in ['carga_provincia_nombre', 'residencia_provincia_nombre']: df_caba.pop(columna)\n",
    "    \n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['residencia_departamento_nombre'])], axis=1)\n",
    "for columna in ['residencia_departamento_nombre', 'SIN ESPECIFICAR']: df_caba.pop(columna)\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['origen_financiamiento'])], axis=1)\n",
    "for columna in ['origen_financiamiento']: df_caba.pop(columna)\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['clasificacion_resumen'])], axis=1)\n",
    "for columna in ['clasificacion_resumen']: df_caba.pop(columna) \n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['clasificacion'])], axis=1)\n",
    "for columna in ['clasificacion']: df_caba.pop(columna) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos las que serán nuestras columnas de control.\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['cuidado_intensivo'])], axis=1)\n",
    "for columna in ['cuidado_intensivo', 'NO']: df_caba.pop(columna)\n",
    "df_caba.rename(columns = {'SI':'cuidado_intensivo'}, inplace = True)\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['asistencia_respiratoria_mecanica'])], axis=1)\n",
    "for columna in ['asistencia_respiratoria_mecanica', 'NO']: df_caba.pop(columna)\n",
    "df_caba.rename(columns = {'SI':'asistencia_respiratoria_mecanica'}, inplace = True)\n",
    "\n",
    "df_caba = pd.concat([df_caba, pd.get_dummies(df_caba['fallecido'])], axis=1)\n",
    "for columna in ['fallecido', 'NO']: df_caba.pop(columna)\n",
    "df_caba.rename(columns = {'SI':'fallecido'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "\n",
    "df_caba = df_caba.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360877, 68)\n",
      "\n",
      "edad <class 'numpy.float64'>(123)[0 nulls]:\n",
      "[53. 69.  7. 46. 29. 61. 43. 38. 75. 55. 39. 48. 37. 50. 31. 47. 42. 35.\n",
      " 54. 49.]\n",
      "\n",
      "sepi_apertura <class 'numpy.int64'>(34)[0 nulls]:\n",
      "[23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  5  6]\n",
      "\n",
      "Femenino <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Masculino <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "residencia_Argentina <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "Buenos Aires <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "CABA <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Catamarca <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Chaco <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Chubut <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Corrientes <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Córdoba <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Entre Ríos <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Formosa <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Jujuy <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "La Pampa <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "La Rioja <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Mendoza <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Misiones <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Neuquén <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Río Negro <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Salta <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "San Juan <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "San Luis <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Santa Cruz <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Santa Fe <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Santiago del Estero <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Tierra del Fuego <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Tucumán <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 01 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 02 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 03 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 04 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 05 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 06 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 07 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 08 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 09 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 10 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 11 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 12 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 13 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 14 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "COMUNA 15 <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Privado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "Público <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Confirmado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Descartado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "Sin Clasificar <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Sospechoso <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso Descartado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[1 0]\n",
      "\n",
      "Caso Invalidado Epidemiologicamente <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso Sospechoso - Muestra no apta <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso Sospechoso - Sin muestra <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por criterio clínico - epidemiológico -  Activo internado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por criterio clínico-epidemiologico - Fallecido <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por criterio clínico-epidemiológico  - No activo (por tiempo de evolución) <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por laboratorio - Activo <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por laboratorio - Activo Internado <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por laboratorio - Fallecido <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por laboratorio - No Activo por criterio de laboratorio <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso confirmado por laboratorio - No activo (por tiempo de evolución) <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Caso sospechoso - Con muestra sin resultado concluyente <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Otro diagnostico <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "Sin clasificar <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "cuidado_intensivo <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "asistencia_respiratoria_mecanica <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n",
      "\n",
      "fallecido <class 'numpy.uint8'>(2)[0 nulls]:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Describimos los features que usaremos para nuestro estudio.\n",
    "# * Shape del dataset\n",
    "# * nombre de la columna\n",
    "# * Type de la variable\n",
    "# * Número de valores únicos\n",
    "# * Número de Nulls\n",
    "# * Muestra de los primeros 20 valores únicos\n",
    "\n",
    "print(df_caba.shape)\n",
    "for col in df_caba.keys():\n",
    "    print('\\n' + col + ' ' + str(type(df_caba[col][df_caba[col].first_valid_index()])) + '(' + str(len(df_caba[col].unique())) + ')[' + str(df_caba[col].isnull().sum()) + ' nulls]:')\n",
    "    print(str(df_caba[col].unique()[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestros datos en función de los features que usaremos para agrupar nuestras muestras.\n",
    "\n",
    "X = df_caba[list(df_caba.columns[:-3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestros datos que usaremos como test para probar si existe relación con nuestros clusters.\n",
    "# Reservaremos para esto: [cuidado_intensivo, asistencia_respiratoria_mecanica, fallecido]\n",
    "\n",
    "Y = df_caba[list(df_caba.columns[-3:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escalamos nuestros datos.\n",
    "\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaler.fit(X)\n",
    "\n",
    "X_std = scaler.transform(X)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizamos nuestros datos luego del autoscaling\n",
    "\n",
    "sns.boxplot(data = pd.DataFrame(X_std))\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"Valor luego de autoscaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el número de componentes principales en nuestro PCA en función del parámetro inicial 'preproc'.\n",
    "n_comps = X_std.shape[1] if preproc == 'raw' else preproc\n",
    "\n",
    "# Definimos nuestra decomposición PCA.\n",
    "decomp_pca = PCA(n_components=n_comps, random_state=0)\n",
    "\n",
    "# Transformamos nuestros datos escalados a nuestro nuevo espacio muestral.\n",
    "pca_data = decomp_pca.fit_transform(X_std)\n",
    "\n",
    "# Generamos nuestro nuevo DataFrame para trabajar.\n",
    "xpca = pd.DataFrame(pca_data)\n",
    "\n",
    "# Generamos nuesotrs eigenvalues, varianza y varianza acumulada para graficar más adelante.\n",
    "eigenvalues = decomp_pca.explained_variance_ratio_\n",
    "percentage_var_explained = decomp_pca.explained_variance_ / np.sum(decomp_pca.explained_variance_);\n",
    "cum_var_explained = np.cumsum(percentage_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos la participación acumulada de la varianza total de nuestros nuevos features.\n",
    "\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.clf()\n",
    "plt.plot(cum_var_explained, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative_explained_variance')\n",
    "plt.savefig('clustering_' + str(preproc) + 'pre_cumulative.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos un Barplot con las features generadas por nuestro PCA.\n",
    "\n",
    "components = range(1,n_comps + 1)\n",
    "plt.bar(components,eigenvalues)\n",
    "plt.xticks(components)\n",
    "plt.title('Explained variance of top ' + str(n_comps) + ' principal components')\n",
    "plt.xlabel('Top ' + str(n_comps) + ' Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.savefig('clustering_' + str(preproc) + 'pre_bars.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos un Scatterplot de nuestros samples en las dos primeras dimensiones de nuestro PCA.\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(xpca.loc[:,0],xpca.loc[:,1], alpha = 0.2)    \n",
    "plt.xlabel('Principal Component 1 , Explainded Variance Ratio = ' + str(np.round(eigenvalues[0],2)))\n",
    "plt.ylabel('Principal Component 2 , Explainded Variance Ratio = ' + str(np.round(eigenvalues[1],2)))\n",
    "plt.title('Principal Component Analysis before feature selection')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_2pca_scatter.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un modelo de Kmeans de clusters con los datos autoscalados con StandardScaler.\n",
    "\n",
    "datos = X_std if preproc == 'raw' else xpca # X_std #xpca\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos cuantos samples asignamos en cada cluster.\n",
    "\n",
    "for label, counts in zip(list(np.unique(kmeans.labels_, return_counts=True)[0]), list(np.unique(kmeans.labels_, return_counts=True)[1])):\n",
    "    print('Cluster ' + str(label) + ': ' + str(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el mismo PCA que antes con solo dos dimensiones para identificar en un gráfico los nuevos clusters.\n",
    "\n",
    "n_comps = 2\n",
    "kmeans_pca = PCA(n_components=n_comps, random_state=0)\n",
    "kmeans_xpca = pd.DataFrame(kmeans_pca.fit_transform(X_std))\n",
    "kmeans_eigenvalues = kmeans_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloreamos nuestros samples en el espacio de las dos primeras dimensiones obtenidas del PCA.\n",
    "\n",
    "y_pca = np.reshape(np.array(kmeans.labels_),(len(kmeans.labels_),1))\n",
    "handlers = [('Cluster ' + str(i)) for i in range(n_clusters)]\n",
    "col = np.zeros((len(y_pca[:,0]),4))\n",
    "col_float = 1.0 / n_clusters\n",
    "arrows = [plt.arrow(0,0, 0,0, head_width=0.0, color=np.reshape(cm.rainbow(np.linspace(i*col_float, i*col_float, 1)),(4,))) for i in range(n_clusters)]\n",
    "for t in range(len(y_pca[:,0])):\n",
    "    ray = y_pca[t,0] * col_float\n",
    "    if y_pca[t,0]==0:\n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "    elif y_pca[t,0]==1: \n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "    elif y_pca[t,0]==2:\n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "    elif y_pca[t,0]==3:\n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "    elif y_pca[t,0]==4:\n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "    elif y_pca[t,0]==5:\n",
    "        col[t,] += np.reshape(cm.rainbow(np.linspace(ray, ray, 1)),(4,))\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(kmeans_xpca.loc[:,0],kmeans_xpca.loc[:,1], c=col, alpha = 0.2)    \n",
    "plt.xlabel('Principal Component 1 , Explainded Variance Ratio = ' + str(np.round(kmeans_eigenvalues[0],2)))\n",
    "plt.ylabel('Principal Component 2 , Explainded Variance Ratio = ' + str(np.round(kmeans_eigenvalues[1],2)))\n",
    "plt.title('Principal Component Analysis after clustering')\n",
    "plt.legend(arrows, handlers, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Clustering_' + str(preproc) + 'pre-' + str(n_clusters) + 'clu.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos nuestra lista de labels a un dataset con una columna por cada label.\n",
    "\n",
    "kmean_labels = pd.DataFrame(kmeans.labels_)\n",
    "clusters_df = pd.get_dummies(kmean_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para facilitar la evaluación de resultados el notebook genera un archivo .csv tipo log con las métricas de\n",
    "# cada experimento.\n",
    "\n",
    "metrics_file = 'cluster_metrics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el Silhouette Score de nuestro experimento.\n",
    "# Siendo el dataset original de 360000+ samples, elegimos aleatoriamente una muestra representativa de 50000.\n",
    "\n",
    "silhouette = silhouette_score(datos, kmean_labels, metric='euclidean', sample_size=50000, random_state=1000)\n",
    "with open(metrics_file, 'a') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([preproc, n_clusters, 'silhouette', format(silhouette, '.4f')])\n",
    "    count_row = [preproc, n_clusters, 'counts']\n",
    "\n",
    "print('Silhouette score: ' + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos en nuestro archivo csv el recuento de samples en cada cluster.\n",
    "\n",
    "count_row = [preproc, n_clusters, 'counts']\n",
    "for count in list(np.unique(kmeans.labels_, return_counts=True)[1]):\n",
    "    count_row = count_row + [str(count)]\n",
    "\n",
    "with open(metrics_file, 'a') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(count_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación se evalúa caso por caso la compatibilidad de nuestros clusters con nuestros datos de control.\n",
    "# * cuidado_intensivo\n",
    "# * asistencia_respiratoria_mecanica\n",
    "# * fallecido\n",
    "#\n",
    "# Para esto usamos tres métricas propias de clustering y problemas de clasificación.\n",
    "# * Accuracy: Equivalente al Rand Index de cada cluster. Expresa cuantas coincidencias existen entre los samples\n",
    "#   en nuestro cluster y los samples positivos en nuestros datos de control.\n",
    "# * Sensitivity: Mide el porcentaje de los casos positivos totales en nuestros datos de control que se encuentran\n",
    "#   asignados a cada uno de nuestros clusters.\n",
    "# * Overlap: Mide el porcentaje de la población de nuestro cluster que puede ser también clasificado como\n",
    "#   positivo en nuestros datos de control. Es una métrica generada exclusivamente para este propósito. Su\n",
    "#   nombre no responde a ninguna bibliografía.\n",
    "#\n",
    "# Se usan estas métricas como una adaptación para el fin que se busca en este estudio. Los casos positivos en\n",
    "# nuestros datos de control son un porcentaje minúsculo y se busca tener en cuenta cada sample en la métrica del\n",
    "# cluster al que pertenece exclusivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas comparativas con los samples que clasificaron para Cuidado Intensivo.\n",
    "\n",
    "ground_truth = 'cuidado_intensivo'\n",
    "metrics_row = [preproc, n_clusters, ground_truth]\n",
    "positives_row = [preproc, n_clusters, ground_truth]\n",
    "\n",
    "for cluster in clusters_df.columns:\n",
    "    y_true = Y[ground_truth]\n",
    "    y_pred = clusters_df[cluster]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    overlap = tp / (tp + fp)\n",
    "\n",
    "    print('Cluster Nº' + str(cluster) + ':')\n",
    "    print('\\tAccuracy:\\t' + str(accuracy) + '\\t[' + str(tn+tp) + ']')\n",
    "    print('\\tSensitivity:\\t' + str(sensitivity) + '\\t[' + str(tp) + ']')\n",
    "    print('\\tOverlap:\\t' + str(overlap) + '\\t[' + str(tp) + ']')\n",
    "    print('\\n')\n",
    "    \n",
    "    metrics_row = metrics_row + [format(accuracy, '.4f'), format(sensitivity, '.4f'), format(overlap, '.4f')]\n",
    "    positives_row = positives_row + [tp]\n",
    "    \n",
    "with open(metrics_file, 'a') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(metrics_row)\n",
    "    csvwriter.writerow(positives_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas comparativas con los samples que clasificaron para Asistencia Respiratoria Mecánica.\n",
    "\n",
    "ground_truth = 'asistencia_respiratoria_mecanica'\n",
    "metrics_row = [preproc, n_clusters, ground_truth]\n",
    "positives_row = [preproc, n_clusters, ground_truth]\n",
    "\n",
    "for cluster in clusters_df.columns:\n",
    "    y_true = Y[ground_truth]\n",
    "    y_pred = clusters_df[cluster]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    overlap = tp / (tp + fp)\n",
    "\n",
    "    print('Cluster Nº' + str(cluster) + ':')\n",
    "    print('\\tAccuracy:\\t' + str(accuracy) + '\\t[' + str(tn+tp) + ']')\n",
    "    print('\\tSensitivity:\\t' + str(sensitivity) + '\\t[' + str(tp) + ']')\n",
    "    print('\\tOverlap:\\t' + str(overlap) + '\\t[' + str(tp) + ']')\n",
    "    print('\\n')\n",
    "    \n",
    "    metrics_row = metrics_row + [format(accuracy, '.4f'), format(sensitivity, '.4f'), format(overlap, '.4f')]\n",
    "    positives_row = positives_row + [tp]\n",
    "    \n",
    "with open(metrics_file, 'a') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(metrics_row)\n",
    "    csvwriter.writerow(positives_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas comparativas con los samples que clasificaron como Fallecido o Fallecida.\n",
    "\n",
    "ground_truth = 'fallecido'\n",
    "metrics_row = [preproc, n_clusters, ground_truth]\n",
    "positives_row = [preproc, n_clusters, ground_truth]\n",
    "\n",
    "for cluster in clusters_df.columns:\n",
    "    y_true = Y[ground_truth]\n",
    "    y_pred = clusters_df[cluster]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    overlap = tp / (tp + fp)\n",
    "\n",
    "    print('Cluster Nº' + str(cluster) + ':')\n",
    "    print('\\tAccuracy:\\t' + str(accuracy) + '\\t[' + str(tn+tp) + ']')\n",
    "    print('\\tSensitivity:\\t' + str(sensitivity) + '\\t[' + str(tp) + ']')\n",
    "    print('\\tOverlap:\\t' + str(overlap) + '\\t[' + str(tp) + ']')\n",
    "    print('\\n')\n",
    "    \n",
    "    metrics_row = metrics_row + [format(accuracy, '.4f'), format(sensitivity, '.4f'), format(overlap, '.4f')]\n",
    "    positives_row = positives_row + [tp]\n",
    "    \n",
    "with open(metrics_file, 'a') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(metrics_row)\n",
    "    csvwriter.writerow(positives_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
